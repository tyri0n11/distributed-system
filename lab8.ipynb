{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyri0n11/distributed-system/blob/main/lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I. Theoretical session:\n",
        "1. Could you list out some limitation of MapReduce?\n",
        "2. Provide a high level comparison of Apache Hadoop and Apache Spark.\n",
        "3. What are the advantages of Apache Spark?\n",
        "4. Provide a comparison of RDD and DataFrame in Spark.  "
      ],
      "metadata": {
        "id": "V-S6ggQMmxvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations of MapReduce\n",
        "\n",
        "###Slow because every computation round writes to disk.\n",
        "\n",
        "###No native support for streaming/real-time workloads.\n",
        "\n",
        "###Hard to code; requires manual map + reduce logic.\n",
        "\n",
        "###Poor for machine learning because ML requires iterations.\n",
        "\n",
        "###Fault tolerance is less efficient than Spark’s lineage mechanism."
      ],
      "metadata": {
        "id": "owO49i32dAsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Item            | Takeaway                                           |\n",
        "| --------------- | -------------------------------------------------- |\n",
        "| Speed           | Spark wins due to in-memory                        |\n",
        "| Workload Type   | Hadoop = batch only, Spark = batch + streaming     |\n",
        "| APIs            | Spark much easier (SQL, Python, Scala)             |\n",
        "| Fault tolerance | Both reliable but Spark redo is faster via lineage |\n"
      ],
      "metadata": {
        "id": "tZEbw0pEdG-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advantages of Apache Spark\n",
        "\n",
        "###In-memory computing = 10–100× faster\n",
        "\n",
        "###Unified: supports batch, streaming, ML, graph at once\n",
        "\n",
        "###Developer-friendly with SQL, Python, Scala\n",
        "\n",
        "###Rich ecosystem: MLlib, GraphX, SparkSQL\n",
        "\n",
        "###Smarter failure recovery vs Hadoop"
      ],
      "metadata": {
        "id": "UkqqsUNrdPFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Key difference    | RDD                             | DataFrame           |\n",
        "| ----------------- | ------------------------------- | ------------------- |\n",
        "| Optimization      | None                            | Catalyst optimizer  |\n",
        "| Schema            | Unstructured                    | Structured          |\n",
        "| Use case          | Low-level ops, custom functions | Fast analytics, SQL |\n",
        "| Performance       | Slower                          | Faster              |\n",
        "| Abstraction level | Low                             | High                |\n"
      ],
      "metadata": {
        "id": "QEfWx6m1dXJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "II. You are given a file `appl_stock.csv`, please carry out the following tasks:\n",
        "\n",
        "1. Read this file by PySpark. Print out the schema.\n",
        "2. Create new columns of combining the High, Low, Close and Adj Close as follow `[High, Low, Close, Adj Close]`.\n",
        "3. Create a new column which computes the average price of High and Low prices.\n",
        "4. Create a new column which computes the amount of money based on the formula `Volume * Adj Close`.\n",
        "3. Using `groupby` and `year()` function to compute the average closing price per year.\n"
      ],
      "metadata": {
        "id": "S9_563ulpsh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder.appName('appl_stock').getOrCreate()\n",
        "df = spark.read.csv('appl_stock.csv', inferSchema=True, header=True)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lykim84kdfW-",
        "outputId": "889e1b6f-7d92-4cdd-d810-3d31c2b0fb0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Adj Close: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('Combination of H L C AdjC', (df['High'] + df['Low'] + df['Close'] + df['Adj Close'])).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBkFJb5-d47p",
        "outputId": "21e32f73-a632-45b2-e721-b52c89964d0a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+------------------+------------------+---------+------------------+-------------------------+\n",
            "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|Combination of H L C AdjC|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+-------------------------+\n",
            "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|               668.617034|\n",
            "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|               670.994957|\n",
            "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|               664.283177|\n",
            "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|        658.9126610000001|\n",
            "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|                660.50405|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+-------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('Amount', (df['Volume'] * df['Adj Close'])).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aLwtvQmeELu",
        "outputId": "840feffb-4aa7-4cab-ef15-c248e7beb791"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+------------------+------------------+---------+------------------+--------------------+\n",
            "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|              Amount|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+--------------------+\n",
            "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|   3.4224149686636E9|\n",
            "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|4.1794728435712004E9|\n",
            "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|3.7730718911200004E9|\n",
            "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|     3.25435088342E9|\n",
            "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|3.0732995574918003E9|\n",
            "+----------+----------+----------+------------------+------------------+---------+------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(year(df['Date'])).avg('Close').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqRxGZQPeYSA",
        "outputId": "b05aef47-57ed-49f0-95cf-92ed75b68ddd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|year(Date)|        avg(Close)|\n",
            "+----------+------------------+\n",
            "|      2015|120.03999980555547|\n",
            "|      2013| 472.6348802857143|\n",
            "|      2014| 295.4023416507935|\n",
            "|      2012| 576.0497195640002|\n",
            "|      2016|104.60400786904763|\n",
            "+----------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "III. You are given a data `customer_churn.csv`, which describes the churn status in clients of a marletting agency. As a data scientist, you are required to create a machine learning model **in Spark** that will help predict which customers will churn (stop buying their service). A short description of the data is as follow:\n",
        "```\n",
        "Name : Name of the latest contact at Company\n",
        "Age: Customer Age\n",
        "Total_Purchase: Total Ads Purchased\n",
        "Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
        "Years: Totaly Years as a customer\n",
        "Num_sites: Number of websites that use the service.\n",
        "Onboard_date: Date that the name of the latest contact was onboarded\n",
        "Location: Client HQ Address\n",
        "Company: Name of Client Company\n",
        "```\n",
        "\n",
        "1. Read, print the schema and check out the data to set the first sight of the data.\n",
        "2. Format the data according to `VectorAssembler`, which is supported in MLlib of PySpark.\n",
        "3. Split the data into train/test data, and then fit train data to the logistic regression model.\n",
        "4. Evaluate the results and compute the AUC."
      ],
      "metadata": {
        "id": "brQ8gRaUshXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Spark = SparkSession.builder.appName('Customer Churn').getOrCreate()\n",
        "df = Spark.read.csv('customer_churn.csv', inferSchema=True, header=True)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "fN4Zb88PtzCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e94c9a7a-1bb6-4122-e709-48cc921cf322"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Names: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- Total_Purchase: double (nullable = true)\n",
            " |-- Account_Manager: integer (nullable = true)\n",
            " |-- Years: double (nullable = true)\n",
            " |-- Num_Sites: double (nullable = true)\n",
            " |-- Onboard_date: timestamp (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Company: string (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Define the input columns for the VectorAssembler\n",
        "input_cols = ['Age', 'Total_Purchase', 'Account_Manager', 'Years', 'Num_Sites']\n",
        "\n",
        "# Create a VectorAssembler instance\n",
        "assembler = VectorAssembler(inputCols=input_cols, outputCol='features')\n",
        "\n",
        "# Transform the DataFrame to include the 'features' column\n",
        "output = assembler.transform(df)\n",
        "\n",
        "# Print the schema and show the first few rows with the new 'features' column\n",
        "output.printSchema()\n",
        "output.select('features', 'Churn').show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBp0QLDJfe-n",
        "outputId": "c91bec67-7d7e-4286-a314-2e50f79a3991"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Names: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- Total_Purchase: double (nullable = true)\n",
            " |-- Account_Manager: integer (nullable = true)\n",
            " |-- Years: double (nullable = true)\n",
            " |-- Num_Sites: double (nullable = true)\n",
            " |-- Onboard_date: timestamp (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Company: string (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n",
            "+-----------------------------+-----+\n",
            "|features                     |Churn|\n",
            "+-----------------------------+-----+\n",
            "|[42.0,11066.8,0.0,7.22,8.0]  |1    |\n",
            "|[41.0,11916.22,0.0,6.5,11.0] |1    |\n",
            "|[38.0,12884.75,0.0,6.67,12.0]|1    |\n",
            "|[42.0,8010.76,0.0,6.71,10.0] |1    |\n",
            "|[37.0,9191.58,0.0,5.56,9.0]  |1    |\n",
            "+-----------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split data to train/test\n",
        "train_data, test_data = output.randomSplit([0.7, 0.3])\n"
      ],
      "metadata": {
        "id": "F2JPrcUUfzs2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use logistic regression to evaluate results\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression(featuresCol='features', labelCol='Churn')\n",
        "\n",
        "fitted_model = lr_model.fit(train_data)"
      ],
      "metadata": {
        "id": "b9GhWSuCf6ij"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_model.summary.predictions.show(5)\n",
        "auc_logis = fitted_model.summary.areaUnderROC\n",
        "print(\"AUC:\", auc_logis)"
      ],
      "metadata": {
        "id": "LjMXChJLgCow",
        "outputId": "5af45bc0-dde8-45c2-b78c-d66f6d6b6778",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|            features|       rawPrediction|         probability|prediction|\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "|      Aaron King|46.0|       7504.79|              0| 5.98|      8.0|2007-05-13 20:42:11|38346 Smith Prair...|       Hernandez PLC|  0.0|[46.0,7504.79,0.0...|[3.06101637843657...|[0.95525575930095...|       0.0|\n",
            "|     Aaron Meyer|45.0|       9598.03|              0|  5.0|      7.0|2010-07-17 03:30:38|35821 Bailey Skyw...|Steele, Bates and...|  0.0|[45.0,9598.03,0.0...|[4.84054570830790...|[0.99215922333793...|       0.0|\n",
            "|      Aaron West|55.0|      10056.55|              0| 4.98|      8.0|2006-09-01 06:11:47|071 Schmidt Locks...|Cruz, Russell and...|  0.0|[55.0,10056.55,0....|[3.23588043338088...|[0.96216241817452...|       0.0|\n",
            "|Abigail Gonzalez|55.0|       8243.28|              0| 3.54|      6.0|2014-01-09 19:52:37|325 Lawrence Cros...|          Nelson PLC|  0.0|[55.0,8243.28,0.0...|[6.35311922077211...|[0.99826171974480...|       0.0|\n",
            "|     Adam Harris|44.0|       9815.03|              1|  4.9|      9.0|2016-05-29 06:00:09|40488 Michael For...|Smith, Oconnor an...|  0.0|[44.0,9815.03,1.0...|[2.06283628409339...|[0.88723824215984...|       0.0|\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "AUC: 0.9023861899884125\n"
          ]
        }
      ]
    }
  ]
}