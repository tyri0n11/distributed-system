{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyri0n11/distributed-system/blob/main/7_2_data_preparation_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6H-nouwaeyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c94f8e99-7b38-47e4-9244-20eda27b1526"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Connecting to security.ubuntu.com (91.189.91\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Connecting to security.ubuntu.com (91.189.91\r                                                                                                    \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [633 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,512 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,031 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,277 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,538 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,302 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,550 kB]\n",
            "Fetched 9,077 kB in 2s (4,411 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGwEF300L3KH",
        "outputId": "1e25e13a-1a55-4b14-92ad-b5d39205fa88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef3J63Uke14M"
      },
      "source": [
        "# `StringIndexer` and `OneHotEncoder`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UJgVwv-atT8"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkContext(conf=SparkConf())\n",
        "spark = SparkSession(sparkContext=sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m37etkvQa1bj"
      },
      "source": [
        "# Example data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyJ5XVFGatqv",
        "outputId": "53bfc5de-3c62-4353-f19f-df7be5083869"
      },
      "source": [
        "import pandas as pd\n",
        "pdf = pd.DataFrame({\n",
        "        'x1': ['a','a','b','b', 'b', 'c'],\n",
        "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach'],\n",
        "        'x3': [1, 1, 2, 2, 2, 4],\n",
        "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5],\n",
        "        'y1': [1, 0, 1, 0, 0, 1],\n",
        "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes']\n",
        "    })\n",
        "# `pdf` is pandas dataframe while `df` is Spark dataframe\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+\n",
            "| x1|    x2| x3| x4| y1| y2|\n",
            "+---+------+---+---+---+---+\n",
            "|  a| apple|  1|2.4|  1|yes|\n",
            "|  a|orange|  1|2.5|  0| no|\n",
            "|  b|orange|  2|3.5|  1| no|\n",
            "|  b|orange|  2|1.4|  0|yes|\n",
            "|  b| peach|  2|2.1|  0|yes|\n",
            "|  c| peach|  4|1.5|  1|yes|\n",
            "+---+------+---+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Type of pdf', type(pdf))\n",
        "print('Type of df', type(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgtkzuwks7DI",
        "outputId": "8c65060f-1fd4-4280-9e56-cac6eb9fc99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of pdf <class 'pandas.core.frame.DataFrame'>\n",
            "Type of df <class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rom_jfRSa9dl"
      },
      "source": [
        "# StringIndexer\n",
        "\n",
        "`StringIndexer` maps a string column to a index column that will be treated as a categorical column by spark. **The indices start with 0 and are ordered by label frequencies**. If it is a numerical column, the column will first be casted to a string column and then indexed by  StringIndexer.\n",
        "\n",
        "There are three steps to implement the StringIndexer\n",
        "\n",
        "1. Build the StringIndexer model: specify the input column and output column names.\n",
        "2. Learn the StringIndexer model: fit the model with your data.\n",
        "3. Execute the indexing: call the transform function to execute the indexing process.\n",
        "\n",
        "### Example: `StringIndex` column \"x1\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr74TF0Oattg",
        "outputId": "798ab37b-ba60-4888-afb5-dd5d3f179f5b"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# build indexer\n",
        "string_indexer = StringIndexer(inputCol='x1', outputCol='indexed_x1')\n",
        "\n",
        "# learn the model\n",
        "string_indexer_model = string_indexer.fit(df)\n",
        "\n",
        "# transform the data\n",
        "df_stringindexer = string_indexer_model.transform(df)\n",
        "\n",
        "# resulting df\n",
        "df_stringindexer.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+----------+\n",
            "| x1|    x2| x3| x4| y1| y2|indexed_x1|\n",
            "+---+------+---+---+---+---+----------+\n",
            "|  a| apple|  1|2.4|  1|yes|       1.0|\n",
            "|  a|orange|  1|2.5|  0| no|       1.0|\n",
            "|  b|orange|  2|3.5|  1| no|       0.0|\n",
            "|  b|orange|  2|1.4|  0|yes|       0.0|\n",
            "|  b| peach|  2|2.1|  0|yes|       0.0|\n",
            "|  c| peach|  4|1.5|  1|yes|       2.0|\n",
            "+---+------+---+---+---+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your task `StringIndex` column \"x2\""
      ],
      "metadata": {
        "id": "-lgrM5cevD2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "# build indexer\n",
        "string_indexer_2 = StringIndexer(inputCol='x2', outputCol='indexed_x2')\n",
        "# learn the model\n",
        "string_indexer_model_2 = string_indexer_2.fit(df)\n",
        "# transform the data\n",
        "df_stringindexer = string_indexer_model_2.transform(df)\n",
        "# resulting df\n",
        "df_stringindexer.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQIB28uBvKdm",
        "outputId": "8777c40f-2b20-46d1-debb-d1f7746b924f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+----------+\n",
            "| x1|    x2| x3| x4| y1| y2|indexed_x2|\n",
            "+---+------+---+---+---+---+----------+\n",
            "|  a| apple|  1|2.4|  1|yes|       2.0|\n",
            "|  a|orange|  1|2.5|  0| no|       0.0|\n",
            "|  b|orange|  2|3.5|  1| no|       0.0|\n",
            "|  b|orange|  2|1.4|  0|yes|       0.0|\n",
            "|  b| peach|  2|2.1|  0|yes|       1.0|\n",
            "|  c| peach|  4|1.5|  1|yes|       1.0|\n",
            "+---+------+---+---+---+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAwN0gu1vlh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q09eR6hNbJ4X"
      },
      "source": [
        "From the result above, we can see that (a, b, c) in column x1 are converted to (1.0, 0.0, 2.0). They are ordered by their frequencies in column x1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-aFv1K_bJ7H"
      },
      "source": [
        "# OneHotEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VhH9d-ybJ-N"
      },
      "source": [
        "**`OneHotEncoder`** converts each categories of a **StringIndexed** column to a `sparse vector`. Each sparse vector has **at most one single active elements** that indicate the category index."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7zMQ4Oyv7UK",
        "outputId": "e86f606c-d98e-4d77-ee2c-db0e8cca5aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+\n",
            "| x1|    x2| x3| x4| y1| y2|\n",
            "+---+------+---+---+---+---+\n",
            "|  a| apple|  1|2.4|  1|yes|\n",
            "|  a|orange|  1|2.5|  0| no|\n",
            "|  b|orange|  2|3.5|  1| no|\n",
            "|  b|orange|  2|1.4|  0|yes|\n",
            "+---+------+---+---+---+---+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "or9TkUeXatvv",
        "outputId": "6dd9c050-8a20-43ea-d768-875ff8fd7a64"
      },
      "source": [
        "df_ohe = df.select('x1')\n",
        "df_ohe.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| x1|\n",
            "+---+\n",
            "|  a|\n",
            "|  a|\n",
            "|  b|\n",
            "|  b|\n",
            "|  b|\n",
            "|  c|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFm04i8AbTfY"
      },
      "source": [
        "### `StringIndex` column 'x1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D21fB0GatzE",
        "outputId": "9d12b52f-681c-4bc1-f19a-5871f2c0b4dc"
      },
      "source": [
        "df_x1_indexed = StringIndexer(inputCol='x1', outputCol='indexed_x1').fit(df_ohe).transform(df_ohe)\n",
        "df_x1_indexed.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "| x1|indexed_x1|\n",
            "+---+----------+\n",
            "|  a|       1.0|\n",
            "|  a|       1.0|\n",
            "|  b|       0.0|\n",
            "|  b|       0.0|\n",
            "|  b|       0.0|\n",
            "|  c|       2.0|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsHb_WbJbYUn"
      },
      "source": [
        "'x1' has three categories: 'a', 'b' and 'c',  which corresponding string indices 1.0, 0.0 and 2.0, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A26L03v2bYXQ"
      },
      "source": [
        "### Mapping string indices to sparse vectors\n",
        "\n",
        "* Encoding format: 'string index': ['string indices vector size', 'index of string index in string indices vector', **1.0** ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuFjx-kObYai"
      },
      "source": [
        "Here the string indices vector is `[0.0, 1.0, 2.0]`. Therefore, the mapping between string indices and sparse vectors are:\n",
        "* `0.0: [3, [0], [1.0]]`\n",
        "* `1.0: [3, [1], [1.0]]`\n",
        "* `2.0: [3, [2], [1.0]]`\n",
        "\n",
        "After we convert all sparse vectors to dense vectors, we get:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC9zDWhMat1R",
        "outputId": "0cebc19d-63fb-4cac-9b6c-23ee85e3417f"
      },
      "source": [
        "from pyspark.ml.linalg import DenseVector, SparseVector, DenseMatrix, SparseMatrix\n",
        "x = [SparseVector(3, {0: 1.0}).toArray()] + \\\n",
        "    [SparseVector(3, {1: 1.0}).toArray()] + \\\n",
        "    [SparseVector(3, {2: 1.0}).toArray()]\n",
        "\n",
        "import numpy as np\n",
        "np.array(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVbgtDBzbpTs"
      },
      "source": [
        "**The obtained matrix is exactly the matrix that we would use to represent our categorical variable in a statistical class**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDYvaXZEbpWq"
      },
      "source": [
        "### One more step to go\n",
        "\n",
        "`OneHotEncoder` by default will drop the last category. So the **string indices vector** becomes `[0.0, 1.0]`, and the mappings between string indices and sparse vectors are:\n",
        "\n",
        "* `0.0: [2, [0], [1.0]]`\n",
        "* `1.0: [2, [1], [1.0]]`\n",
        "* `2.0: [2, [], []]`\n",
        "\n",
        "We use a sparse vector that has **no active element**(basically all elements are 0's) to represent the last category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws0oqTKkbpZK"
      },
      "source": [
        "# Verify\n",
        "\n",
        "### OneHotEncode column 'indexed_x1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZuADf7iat8D"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# review `df_x1_indexed`, what is it?\n",
        "df_x1_indexed.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFOkG6uQ652t",
        "outputId": "b0555ce4-eb41-468e-ca11-57103f6d59ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "| x1|indexed_x1|\n",
            "+---+----------+\n",
            "|  a|       1.0|\n",
            "|  a|       1.0|\n",
            "|  b|       0.0|\n",
            "|  b|       0.0|\n",
            "|  b|       0.0|\n",
            "+---+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UozxlA0cb3Nm",
        "outputId": "f44cfad3-3dc0-4214-e5df-0047c7957e82"
      },
      "source": [
        "OneHotEncoder(inputCol='indexed_x1', outputCol='encoded_x1').fit(df_x1_indexed).transform(df_x1_indexed).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-------------+\n",
            "| x1|indexed_x1|   encoded_x1|\n",
            "+---+----------+-------------+\n",
            "|  a|       1.0|(2,[1],[1.0])|\n",
            "|  a|       1.0|(2,[1],[1.0])|\n",
            "|  b|       0.0|(2,[0],[1.0])|\n",
            "|  b|       0.0|(2,[0],[1.0])|\n",
            "|  b|       0.0|(2,[0],[1.0])|\n",
            "|  c|       2.0|    (2,[],[])|\n",
            "+---+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YITbqgAgecCF"
      },
      "source": [
        "### Specify to not drop the last category\n",
        "\n",
        "If we choose to not drop the last category, we get the expected results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6y1gArnb3Qw",
        "outputId": "220561ba-fb26-45cc-b95f-31a68d04786e"
      },
      "source": [
        "OneHotEncoder(dropLast=False, inputCol='indexed_x1', outputCol='encoded_x1').fit(df_x1_indexed).transform(df_x1_indexed).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-------------+\n",
            "| x1|indexed_x1|   encoded_x1|\n",
            "+---+----------+-------------+\n",
            "|  a|       1.0|(3,[1],[1.0])|\n",
            "|  a|       1.0|(3,[1],[1.0])|\n",
            "|  b|       0.0|(3,[0],[1.0])|\n",
            "|  b|       0.0|(3,[0],[1.0])|\n",
            "|  b|       0.0|(3,[0],[1.0])|\n",
            "|  c|       2.0|(3,[2],[1.0])|\n",
            "+---+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise:\n",
        "**Do the same OneHotEncoder for the columns `x2` and `y2`**"
      ],
      "metadata": {
        "id": "OpfroL2gf3bY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw65oYrcb3Ss"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddY5V8VXfuxc"
      },
      "source": [
        "# Vector assembler\n",
        "\n",
        "## Example data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph7SsboWb3Vz",
        "outputId": "b3c25f63-35cd-4fbb-8651-29bcd8bac60b"
      },
      "source": [
        "import pandas as pd\n",
        "pdf = pd.DataFrame({\n",
        "        'x1': ['a','a','b','b', 'b', 'c'],\n",
        "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach'],\n",
        "        'x3': [1, 1, 2, 2, 2, 4],\n",
        "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5],\n",
        "        'y1': [1, 0, 1, 0, 0, 1],\n",
        "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes']\n",
        "    })\n",
        "df = spark.createDataFrame(pdf)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+\n",
            "| x1|    x2| x3| x4| y1| y2|\n",
            "+---+------+---+---+---+---+\n",
            "|  a| apple|  1|2.4|  1|yes|\n",
            "|  a|orange|  1|2.5|  0| no|\n",
            "|  b|orange|  2|3.5|  1| no|\n",
            "|  b|orange|  2|1.4|  0|yes|\n",
            "|  b| peach|  2|2.1|  0|yes|\n",
            "|  c| peach|  4|1.5|  1|yes|\n",
            "+---+------+---+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6gUwqSJf8h4"
      },
      "source": [
        "# VectorAssembler\n",
        "\n",
        "To fit a ML model in pyspark, we need to combine all feature columns into one single column of vectors: the **featuresCol**. The `VectorAssembler` can be used to combine multiple **`OneHotEncoder` columns** and **other continuous variable columns** into one single column.\n",
        "\n",
        "The example below shows how to combine three OneHotEncoder columns and one numeric column into a **featureCol** column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp7oT4vvf-WG"
      },
      "source": [
        "## StringIndex and OneHotEncode categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcAhLCUkb3YB"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGqs4THGb3ak",
        "outputId": "94ce67b6-3112-424a-c735-8c9cc037ec96"
      },
      "source": [
        "all_stages = [StringIndexer(inputCol=c, outputCol='idx_' + c) for c in ['x1', 'x2', 'x3']] + \\\n",
        "             [OneHotEncoder(inputCol='idx_' + c, outputCol='ohe_' + c) for c in ['x1', 'x2', 'x3']]\n",
        "all_stages"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StringIndexer_20376647e69c,\n",
              " StringIndexer_3ad93263c2ec,\n",
              " StringIndexer_e31d2f557eb3,\n",
              " OneHotEncoder_5bfbbfd49e10,\n",
              " OneHotEncoder_fccfaa37006a,\n",
              " OneHotEncoder_00432b4231d4]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A92-NF0Pat-m",
        "outputId": "b7c46e84-9e4f-4337-b1af-56822d2e0dad"
      },
      "source": [
        "df_new = Pipeline(stages=all_stages).fit(df).transform(df)\n",
        "df_new.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+------+------+------+-------------+-------------+-------------+\n",
            "| x1|    x2| x3| x4| y1| y2|idx_x1|idx_x2|idx_x3|       ohe_x1|       ohe_x2|       ohe_x3|\n",
            "+---+------+---+---+---+---+------+------+------+-------------+-------------+-------------+\n",
            "|  a| apple|  1|2.4|  1|yes|   1.0|   2.0|   1.0|(2,[1],[1.0])|    (2,[],[])|(2,[1],[1.0])|\n",
            "|  a|orange|  1|2.5|  0| no|   1.0|   0.0|   1.0|(2,[1],[1.0])|(2,[0],[1.0])|(2,[1],[1.0])|\n",
            "|  b|orange|  2|3.5|  1| no|   0.0|   0.0|   0.0|(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|\n",
            "|  b|orange|  2|1.4|  0|yes|   0.0|   0.0|   0.0|(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|\n",
            "|  b| peach|  2|2.1|  0|yes|   0.0|   1.0|   0.0|(2,[0],[1.0])|(2,[1],[1.0])|(2,[0],[1.0])|\n",
            "|  c| peach|  4|1.5|  1|yes|   2.0|   1.0|   2.0|    (2,[],[])|(2,[1],[1.0])|    (2,[],[])|\n",
            "+---+------+---+---+---+---+------+------+------+-------------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K2_vP5qgOhf"
      },
      "source": [
        "## Assemble feature columns into one single **feacturesCol** with **`VectorAssembler`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "023kdI6bgK8L",
        "outputId": "24bf1db2-2a94-4f25-caf8-43662ed21766"
      },
      "source": [
        "df_assembled = VectorAssembler(inputCols=['ohe_x1', 'ohe_x2', 'ohe_x3', 'x4'], outputCol='featuresCol')\\\n",
        "    .transform(df_new)\\\n",
        "    .drop('idx_x1', 'idx_x2', 'idx_x3')\n",
        "df_assembled.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+---+---+-------------+-------------+-------------+-----------------------------+\n",
            "|x1 |x2    |x3 |x4 |y1 |y2 |ohe_x1       |ohe_x2       |ohe_x3       |featuresCol                  |\n",
            "+---+------+---+---+---+---+-------------+-------------+-------------+-----------------------------+\n",
            "|a  |apple |1  |2.4|1  |yes|(2,[1],[1.0])|(2,[],[])    |(2,[1],[1.0])|(7,[1,5,6],[1.0,1.0,2.4])    |\n",
            "|a  |orange|1  |2.5|0  |no |(2,[1],[1.0])|(2,[0],[1.0])|(2,[1],[1.0])|[0.0,1.0,1.0,0.0,0.0,1.0,2.5]|\n",
            "|b  |orange|2  |3.5|1  |no |(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|[1.0,0.0,1.0,0.0,1.0,0.0,3.5]|\n",
            "|b  |orange|2  |1.4|0  |yes|(2,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|[1.0,0.0,1.0,0.0,1.0,0.0,1.4]|\n",
            "|b  |peach |2  |2.1|0  |yes|(2,[0],[1.0])|(2,[1],[1.0])|(2,[0],[1.0])|[1.0,0.0,0.0,1.0,1.0,0.0,2.1]|\n",
            "|c  |peach |4  |1.5|1  |yes|(2,[],[])    |(2,[1],[1.0])|(2,[],[])    |(7,[3,6],[1.0,1.5])          |\n",
            "+---+------+---+---+---+---+-------------+-------------+-------------+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5loOVu0gTb_"
      },
      "source": [
        "## Convert sparse vectors in featuresCol to dense vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBXnLcxQgLZ7"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.linalg import SparseVector, DenseVector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kFoNLNEgLco"
      },
      "source": [
        "def dense_features_col(x):\n",
        "    return(x.toArray().dtype)\n",
        "dense_features_col_udf = udf(dense_features_col, returnType=StringType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8EOR7_PtRVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoHGEeBLgLgw",
        "outputId": "9ffd8f98-d2b3-407c-c198-91e786ef2134"
      },
      "source": [
        "df_assembled.rdd.map(lambda x: x['featuresCol']).take(6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SparseVector(7, {1: 1.0, 5: 1.0, 6: 2.4}),\n",
              " DenseVector([0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.5]),\n",
              " DenseVector([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 3.5]),\n",
              " DenseVector([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.4]),\n",
              " DenseVector([1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.1]),\n",
              " SparseVector(7, {3: 1.0, 6: 1.5})]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtYLdUhogZmB",
        "outputId": "89d2b6b8-681f-4219-d90a-8682b41d9eb6"
      },
      "source": [
        "df_assembled.rdd.map(lambda x: list(x['featuresCol'].toArray())).take(6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.4],\n",
              " [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.5],\n",
              " [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 3.5],\n",
              " [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.4],\n",
              " [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 2.1],\n",
              " [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.5]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef9g5bCrgbnn"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice"
      ],
      "metadata": {
        "id": "pbrgMGKa1kWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pdf2 = pd.DataFrame({\n",
        "        'x1': ['a','a','b','b', 'b', 'c'],\n",
        "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach'],\n",
        "        'x3': [1, 1, 2, 2, 2, 4],\n",
        "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5],\n",
        "        'x5': ['man', 'woman', 'man', 'man', 'man', 'woman'],\n",
        "        'x6': [10.3, 11.4, 45.3, 32.5, 13.8, 17.2],\n",
        "        'x7': ['911', '113', '115', '113', '911', '115'],\n",
        "        'y1': [1, 0, 1, 0, 0, 1],\n",
        "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes']\n",
        "    })\n",
        "df2 = spark.createDataFrame(pdf2)\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQS0WNid1mUX",
        "outputId": "c2971410-064a-42b6-e9d6-6635807b0f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
            "  for column, series in pdf.iteritems():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+---+-----+----+---+---+---+\n",
            "| x1|    x2| x3| x4|   x5|  x6| x7| y1| y2|\n",
            "+---+------+---+---+-----+----+---+---+---+\n",
            "|  a| apple|  1|2.4|  man|10.3|911|  1|yes|\n",
            "|  a|orange|  1|2.5|woman|11.4|113|  0| no|\n",
            "|  b|orange|  2|3.5|  man|45.3|115|  1| no|\n",
            "|  b|orange|  2|1.4|  man|32.5|113|  0|yes|\n",
            "|  b| peach|  2|2.1|  man|13.8|911|  0|yes|\n",
            "|  c| peach|  4|1.5|woman|17.2|115|  1|yes|\n",
            "+---+------+---+---+-----+----+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your task: Please do the Assemble feature columns (of all categorical and numerical features) into one single **feacturesCol** with **`VectorAssembler`**\n",
        "**Hint: Categorical features (`x1`, `x2`, `x3`, `x5`, `x7`) and numerical features (`x4`, `x6`)**"
      ],
      "metadata": {
        "id": "iDtjTiFNleNx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_XGN0nP2OaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise: Do the Assemble feature columns (of all categorical and numerical features) into one single **feacturesCol** with **`VectorAssembler`** for **`mtcars`** dataset"
      ],
      "metadata": {
        "id": "4kuUBb5FqXlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise: Do the Assemble feature columns (of all categorical and numerical features) into one single **feacturesCol** with **`VectorAssembler`** for **`titanic`** dataset"
      ],
      "metadata": {
        "id": "BMAxLQaRoTHE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aeT87zREqhSO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}