{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyri0n11/distributed-system/blob/main/7_2_regression_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32GC-x-Jeohv"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(master = 'local[10]')\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "          .appName(\"Python Spark SQL basic example\") \\\n",
        "          .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "          .getOrCreate()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyJ69dq3e05y"
      },
      "source": [
        "# Linear regression without cross-valiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f1sdMlCfT6b",
        "outputId": "525c87bc-c988-4740-c981-6697dc6c20c7"
      },
      "source": [
        "ad = spark.read.csv('./Advertising.csv', header=True, inferSchema=True)\n",
        "ad.show(5)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+---------+-----+\n",
            "|   TV|Radio|Newspaper|Sales|\n",
            "+-----+-----+---------+-----+\n",
            "|230.1| 37.8|     69.2| 22.1|\n",
            "| 44.5| 39.3|     45.1| 10.4|\n",
            "| 17.2| 45.9|     69.3|  9.3|\n",
            "|151.5| 41.3|     58.5| 18.5|\n",
            "|180.8| 10.8|     58.4| 12.9|\n",
            "+-----+-----+---------+-----+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(ad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "I7ArSC0T4kLy",
        "outputId": "13b1cf85-149b-41cc-c1e0-86debebdea6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.classic.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.classic.dataframe.DataFrame</b><br/>def __init__(jdf: &#x27;JavaObject&#x27;, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg(\n",
              "...         {&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).sort(&quot;max(age)&quot;).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|PySpark|     M|       75.0|      50|\n",
              "|     ML|     F|      150.0|      60|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 105);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NwU4yPcfeW5"
      },
      "source": [
        "## Transform data structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S10xnONfaf6",
        "outputId": "82c89a45-92e0-460c-b8f1-aab9ed6fdde3"
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "ad_df = ad.rdd.map(lambda x: [Vectors.dense(x[0:3]), x[-1]]).toDF(['features', 'label'])\n",
        "ad_df.show(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|         features|label|\n",
            "+-----------------+-----+\n",
            "|[230.1,37.8,69.2]| 22.1|\n",
            "| [44.5,39.3,45.1]| 10.4|\n",
            "| [17.2,45.9,69.3]|  9.3|\n",
            "|[151.5,41.3,58.5]| 18.5|\n",
            "|[180.8,10.8,58.4]| 12.9|\n",
            "+-----------------+-----+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwhoXn3XfkUU"
      },
      "source": [
        "## Build linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whCgAEc8fa55"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "VP65SUAU5Dcz",
        "outputId": "be96071e-7c2b-47ba-9c0d-a16325decd89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.ml.regression.LinearRegression"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.ml.regression.LinearRegression</b><br/>def __init__(*, featuresCol: str=&#x27;features&#x27;, labelCol: str=&#x27;label&#x27;, predictionCol: str=&#x27;prediction&#x27;, maxIter: int=100, regParam: float=0.0, elasticNetParam: float=0.0, tol: float=1e-06, fitIntercept: bool=True, standardization: bool=True, solver: str=&#x27;auto&#x27;, weightCol: Optional[str]=None, aggregationDepth: int=2, loss: str=&#x27;squaredError&#x27;, epsilon: float=1.35, maxBlockSizeInMB: float=0.0)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/ml/regression.py</a>Linear regression.\n",
              "\n",
              "The learning objective is to minimize the specified loss function, with regularization.\n",
              "This supports two kinds of loss:\n",
              "\n",
              "* squaredError (a.k.a squared loss)\n",
              "* huber (a hybrid of squared error for relatively small errors and absolute error for     relatively large ones, and we estimate the scale parameter from training data)\n",
              "\n",
              "This supports multiple types of regularization:\n",
              "\n",
              "* none (a.k.a. ordinary least squares)\n",
              "* L2 (ridge regression)\n",
              "* L1 (Lasso)\n",
              "* L2 + L1 (elastic net)\n",
              "\n",
              ".. versionadded:: 1.4.0\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Fitting with huber loss only supports none and L2 regularization.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; from pyspark.ml.linalg import Vectors\n",
              "&gt;&gt;&gt; df = spark.createDataFrame([\n",
              "...     (1.0, 2.0, Vectors.dense(1.0)),\n",
              "...     (0.0, 2.0, Vectors.sparse(1, [], []))], [&quot;label&quot;, &quot;weight&quot;, &quot;features&quot;])\n",
              "&gt;&gt;&gt; lr = LinearRegression(regParam=0.0, solver=&quot;normal&quot;, weightCol=&quot;weight&quot;)\n",
              "&gt;&gt;&gt; lr.setMaxIter(5)\n",
              "LinearRegression...\n",
              "&gt;&gt;&gt; lr.getMaxIter()\n",
              "5\n",
              "&gt;&gt;&gt; lr.setRegParam(0.1)\n",
              "LinearRegression...\n",
              "&gt;&gt;&gt; lr.getRegParam()\n",
              "0.1\n",
              "&gt;&gt;&gt; lr.setRegParam(0.0)\n",
              "LinearRegression...\n",
              "&gt;&gt;&gt; model = lr.fit(df)\n",
              "&gt;&gt;&gt; model.setFeaturesCol(&quot;features&quot;)\n",
              "LinearRegressionModel...\n",
              "&gt;&gt;&gt; model.setPredictionCol(&quot;newPrediction&quot;)\n",
              "LinearRegressionModel...\n",
              "&gt;&gt;&gt; model.getMaxIter()\n",
              "5\n",
              "&gt;&gt;&gt; model.getMaxBlockSizeInMB()\n",
              "0.0\n",
              "&gt;&gt;&gt; test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [&quot;features&quot;])\n",
              "&gt;&gt;&gt; abs(model.predict(test0.head().features) - (-1.0)) &lt; 0.001\n",
              "True\n",
              "&gt;&gt;&gt; abs(model.transform(test0).head().newPrediction - (-1.0)) &lt; 0.001\n",
              "True\n",
              "&gt;&gt;&gt; bool(abs(model.coefficients[0] - 1.0) &lt; 0.001)\n",
              "True\n",
              "&gt;&gt;&gt; abs(model.intercept - 0.0) &lt; 0.001\n",
              "True\n",
              "&gt;&gt;&gt; test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [&quot;features&quot;])\n",
              "&gt;&gt;&gt; abs(model.transform(test1).head().newPrediction - 1.0) &lt; 0.001\n",
              "True\n",
              "&gt;&gt;&gt; lr.setParams(featuresCol=&quot;vector&quot;)\n",
              "LinearRegression...\n",
              "&gt;&gt;&gt; lr_path = temp_path + &quot;/lr&quot;\n",
              "&gt;&gt;&gt; lr.save(lr_path)\n",
              "&gt;&gt;&gt; lr2 = LinearRegression.load(lr_path)\n",
              "&gt;&gt;&gt; lr2.getMaxIter()\n",
              "5\n",
              "&gt;&gt;&gt; model_path = temp_path + &quot;/lr_model&quot;\n",
              "&gt;&gt;&gt; model.save(model_path)\n",
              "&gt;&gt;&gt; model2 = LinearRegressionModel.load(model_path)\n",
              "&gt;&gt;&gt; bool(model.coefficients[0] == model2.coefficients[0])\n",
              "True\n",
              "&gt;&gt;&gt; bool(model.intercept == model2.intercept)\n",
              "True\n",
              "&gt;&gt;&gt; bool(model.transform(test0).take(1) == model2.transform(test0).take(1))\n",
              "True\n",
              "&gt;&gt;&gt; model.numFeatures\n",
              "1\n",
              "&gt;&gt;&gt; model.write().format(&quot;pmml&quot;).save(model_path + &quot;_2&quot;)</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 211);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbHlDuC_foGi"
      },
      "source": [
        "## Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6CsYKPEfa8Y"
      },
      "source": [
        "lr_model = lr.fit(ad_df)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(lr_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "_57F3iZC5OwQ",
        "outputId": "08d8f600-b7e3-4d66-b352-66dc3114fb1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.ml.regression.LinearRegressionModel"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.ml.regression.LinearRegressionModel</b><br/>def __init__(java_model: Optional[&#x27;JavaObject&#x27;]=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/ml/regression.py</a>Model fitted by :class:`LinearRegression`.\n",
              "\n",
              ".. versionadded:: 1.4.0</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 444);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS7DrR2Y7PMS",
        "outputId": "255c204f-7920-4a9d-9a38-d2eec3a79d24"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegressionModel: uid=LinearRegression_e73692ae1e31, numFeatures=3"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTfxNyntfoyf"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHSnJQ2Qfa_f",
        "outputId": "bda5a26d-d8a7-48de-ecc0-df5e80c53a74"
      },
      "source": [
        "pred = lr_model.transform(ad_df)\n",
        "pred.show(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+------------------+\n",
            "|         features|label|        prediction|\n",
            "+-----------------+-----+------------------+\n",
            "|[230.1,37.8,69.2]| 22.1| 20.52397440971517|\n",
            "| [44.5,39.3,45.1]| 10.4|12.337854820894362|\n",
            "| [17.2,45.9,69.3]|  9.3|12.307670779994238|\n",
            "|[151.5,41.3,58.5]| 18.5| 17.59782951168913|\n",
            "|[180.8,10.8,58.4]| 12.9|13.188671856831299|\n",
            "+-----------------+-----+------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY3-lv0lfpjW"
      },
      "source": [
        "## Module evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PafLoz9kfbB1",
        "outputId": "58e1fbd0-1182-4e42-dc69-d1123be7b63c"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='label')\n",
        "evaluator.setMetricName('r2').evaluate(pred)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.897210638178952"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='label')\n",
        "evaluator.setMetricName('mse').evaluate(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmL61leP7fPm",
        "outputId": "3c0f47ed-a8ed-4816-feb1-cb48ffac9fac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.784126314510938"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.setMetricName('mae').evaluate(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKmbGtxr7mVj",
        "outputId": "470b27a9-2337-45d9-ac12-a14ecb8d3835"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2520112296870693"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "1. Do research in Spark documentaion to study & practice in other regression models.\n",
        "2. Investigate other metrics for evaluating regression model in Spark.\n",
        "3. Practice on `Advertising` dataset.\n",
        "4. Do the same thing, but now you should split into `train` and `test` datasets, so the fitting modelling in the `train`, then do the evaluation in the `test`"
      ],
      "metadata": {
        "id": "mKoDcxNv5s3m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rne-jL6Pf4pC"
      },
      "source": [],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZS95SxIf-OL"
      },
      "source": [
        "# Linear regression with cross-validation in Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szIXpqodf-aC"
      },
      "source": [
        "training, test = ad_df.randomSplit([0.8, 0.2], seed=123)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFfseuBaf-cr"
      },
      "source": [
        "##=====build cross valiation model======\n",
        "\n",
        "# estimator\n",
        "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')\n",
        "\n",
        "# parameter grid\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "param_grid = ParamGridBuilder().\\\n",
        "    addGrid(lr.regParam, [0, 0.5, 1]).\\\n",
        "    addGrid(lr.elasticNetParam, [0, 0.5, 1]).\\\n",
        "    build()\n",
        "\n",
        "# evaluator\n",
        "evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='label', metricName='r2')\n",
        "\n",
        "# cross-validation model\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "cv = CrossValidator(estimator=lr, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=4)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(cv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "NPHMKqsL_9Zv",
        "outputId": "9e67ef64-3e9a-4b06-d251-62d40a3ed967"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.ml.tuning.CrossValidator"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.ml.tuning.CrossValidator</b><br/>def __init__(*, estimator: Optional[Estimator]=None, estimatorParamMaps: Optional[List[&#x27;ParamMap&#x27;]]=None, evaluator: Optional[Evaluator]=None, numFolds: int=3, seed: Optional[int]=None, parallelism: int=1, collectSubModels: bool=False, foldCol: str=&#x27;&#x27;) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/ml/tuning.py</a>K-fold cross validation performs model selection by splitting the dataset into a set of\n",
              "non-overlapping randomly partitioned folds which are used as separate training and test datasets\n",
              "e.g., with k=3 folds, K-fold cross validation will generate 3 (training, test) dataset pairs,\n",
              "each of which uses 2/3 of the data for training and 1/3 for testing. Each fold is used as the\n",
              "test set exactly once.\n",
              "\n",
              ".. versionadded:: 1.4.0\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; from pyspark.ml.classification import LogisticRegression\n",
              "&gt;&gt;&gt; from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
              "&gt;&gt;&gt; from pyspark.ml.linalg import Vectors\n",
              "&gt;&gt;&gt; from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
              "&gt;&gt;&gt; import tempfile\n",
              "&gt;&gt;&gt; dataset = spark.createDataFrame(\n",
              "...     [(Vectors.dense([0.0]), 0.0),\n",
              "...      (Vectors.dense([0.4]), 1.0),\n",
              "...      (Vectors.dense([0.5]), 0.0),\n",
              "...      (Vectors.dense([0.6]), 1.0),\n",
              "...      (Vectors.dense([1.0]), 1.0)] * 10,\n",
              "...     [&quot;features&quot;, &quot;label&quot;])\n",
              "&gt;&gt;&gt; lr = LogisticRegression()\n",
              "&gt;&gt;&gt; grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
              "&gt;&gt;&gt; evaluator = BinaryClassificationEvaluator()\n",
              "&gt;&gt;&gt; cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator,\n",
              "...     parallelism=2)\n",
              "&gt;&gt;&gt; cvModel = cv.fit(dataset)\n",
              "&gt;&gt;&gt; cvModel.getNumFolds()\n",
              "3\n",
              "&gt;&gt;&gt; float(cvModel.avgMetrics[0])\n",
              "0.5\n",
              "&gt;&gt;&gt; path = tempfile.mkdtemp()\n",
              "&gt;&gt;&gt; model_path = path + &quot;/model&quot;\n",
              "&gt;&gt;&gt; cvModel.write().save(model_path)\n",
              "&gt;&gt;&gt; cvModelRead = CrossValidatorModel.read().load(model_path)\n",
              "&gt;&gt;&gt; cvModelRead.avgMetrics\n",
              "[0.5, ...\n",
              "&gt;&gt;&gt; evaluator.evaluate(cvModel.transform(dataset))\n",
              "0.8333...\n",
              "&gt;&gt;&gt; evaluator.evaluate(cvModelRead.transform(dataset))\n",
              "0.8333...</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 676);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY1TWIOSf-f5"
      },
      "source": [
        "cv_model = cv.fit(training)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(cv_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "kvMMJTDWAGx8",
        "outputId": "59006c09-9232-49d4-b719-a15a90723060"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.ml.tuning.CrossValidatorModel"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.ml.tuning.CrossValidatorModel</b><br/>def __init__(bestModel: Model, avgMetrics: Optional[List[float]]=None, subModels: Optional[List[List[Model]]]=None, stdMetrics: Optional[List[float]]=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/ml/tuning.py</a>CrossValidatorModel contains the model with the highest average cross-validation\n",
              "metric across folds and uses this model to transform input data. CrossValidatorModel\n",
              "also tracks the metrics for each param map evaluated.\n",
              "\n",
              ".. versionadded:: 1.4.0\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Since version 3.3.0, CrossValidatorModel contains a new attribute &quot;stdMetrics&quot;,\n",
              "which represent standard deviation of metrics for each paramMap in\n",
              "CrossValidator.estimatorParamMaps.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 1026);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFzzyKyW8i2b",
        "outputId": "43f16e87-bd96-49a7-b509-21087ec53419"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossValidatorModel_dbfd95ffd5cd"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIkZ5GPof-ie"
      },
      "source": [
        "pred_training_cv = cv_model.transform(training)\n",
        "pred_test_cv = cv_model.transform(test)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEopRkEUf-k0",
        "outputId": "fd84d9b8-ae31-4d17-e3c3-da8b0221b724"
      },
      "source": [
        "# performance on training data\n",
        "evaluator.setMetricName('r2').evaluate(pred_training_cv)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8952845631627804"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pztn845f-nf",
        "outputId": "65d781f3-7e64-4a02-cc3b-e992238600a1"
      },
      "source": [
        "# performance on test data\n",
        "evaluator.setMetricName('r2').evaluate(pred_test_cv)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9013819610158472"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OUIp-bsgRT5"
      },
      "source": [
        "## Intercept and coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwVPl4fcgStY",
        "outputId": "e7016d72-fd1f-49c6-fcca-719b435a42bb"
      },
      "source": [
        "print('Intercept: ', cv_model.bestModel.intercept, \"\\n\",\n",
        "     'coefficients: ', cv_model.bestModel.coefficients)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept:  2.9592600706772787 \n",
            " coefficients:  [0.04613729524909818,0.19200356629524312,-0.006269704193266422]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ad_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGn8QEx4XWck",
        "outputId": "b95f6b0a-265e-4c66-ecce-471ebc88fd01"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|         features|label|\n",
            "+-----------------+-----+\n",
            "|[230.1,37.8,69.2]| 22.1|\n",
            "| [44.5,39.3,45.1]| 10.4|\n",
            "| [17.2,45.9,69.3]|  9.3|\n",
            "|[151.5,41.3,58.5]| 18.5|\n",
            "|[180.8,10.8,58.4]| 12.9|\n",
            "+-----------------+-----+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us8cqE8SgYnc"
      },
      "source": [
        "## Get parameter values from the best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV8hCL15gSwb",
        "outputId": "34ad6ddc-5fbe-4fe3-a387-7c561482fb23"
      },
      "source": [
        "print('best regParam: ' + str(cv_model.bestModel._java_obj.getRegParam()) + \"\\n\" +\n",
        "     'best ElasticNetParam:' + str(cv_model.bestModel._java_obj.getElasticNetParam()))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best regParam: 0.0\n",
            "best ElasticNetParam:0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Wrap up the code of building cross-validation models in a Python class"
      ],
      "metadata": {
        "id": "qR6Ju0Sw80Ne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Do the regression to forecast the `inside_sale` of this data: https://github.com/maks-p/restaurant_sales_forecasting/blob/master/csv/CSV_for_EDA_NEW.csv"
      ],
      "metadata": {
        "id": "nYGrazGzD6xT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAjgMl2QgSz1",
        "outputId": "7b90984b-b280-4e77-ca62-2a9b927a6b04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_sales = spark.read.csv('./CSV_for_EDA_NEW.csv', header=True, inferSchema=True)\n",
        "df_sales.show(5)\n",
        "df_sales.printSchema()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+-------------+--------------+---------------+-------------+---------------+--------------+---------------+--------------------+--------+--------------------+-------------------+-----------+-----------+--------+-------------------+-----------+\n",
            "|      date|inside_sales|outside_sales|inside_covers|outside_covers|reserved_covers|walkin_covers|waitlist_covers|no_show_covers|no_show_parties|apparent_temperature|humidity|precip_intensity_max|    precip_max_time|precip_prob|precip_type|pressure|            summary|temperature|\n",
            "+----------+------------+-------------+-------------+--------------+---------------+-------------+---------------+--------------+---------------+--------------------+--------+--------------------+-------------------+-----------+-----------+--------+-------------------+-----------+\n",
            "|2017-01-02|    13159.84|          0.0|          174|             0|            106|           26|             42|            17|              6|               35.58|    0.92|              0.0242|2026-01-03 15:00:00|       0.79|       rain| 1027.98|               rain|      39.06|\n",
            "|2017-01-03|    12442.11|          0.0|          181|             0|            119|           31|             31|            14|              4|                41.5|    0.94|              0.0913|2026-01-03 17:00:00|       0.77|       rain| 1000.08|               rain|      43.19|\n",
            "|2017-01-04|    12927.64|          0.0|          174|             0|            131|           17|             26|             5|              2|               36.81|     0.4|              0.0176|2026-01-03 00:00:00|        0.0|       rain| 1002.55|        clear-night|      42.29|\n",
            "|2017-01-05|    14457.79|          0.0|          191|             0|            138|           25|             28|             4|              2|               27.11|    0.48|                 0.0|2026-01-03 05:01:00|        0.0|       none| 1014.61|             cloudy|      31.36|\n",
            "|2017-01-06|    15331.97|          0.0|          200|             0|            130|           16|             54|             6|              3|               24.74|    0.48|              0.0019|2026-01-03 12:00:00|        0.0|       snow|  1022.8|partly-cloudy-night|      29.42|\n",
            "+----------+------------+-------------+-------------+--------------+---------------+-------------+---------------+--------------+---------------+--------------------+--------+--------------------+-------------------+-----------+-----------+--------+-------------------+-----------+\n",
            "only showing top 5 rows\n",
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- inside_sales: double (nullable = true)\n",
            " |-- outside_sales: double (nullable = true)\n",
            " |-- inside_covers: integer (nullable = true)\n",
            " |-- outside_covers: integer (nullable = true)\n",
            " |-- reserved_covers: integer (nullable = true)\n",
            " |-- walkin_covers: integer (nullable = true)\n",
            " |-- waitlist_covers: integer (nullable = true)\n",
            " |-- no_show_covers: integer (nullable = true)\n",
            " |-- no_show_parties: integer (nullable = true)\n",
            " |-- apparent_temperature: double (nullable = true)\n",
            " |-- humidity: double (nullable = true)\n",
            " |-- precip_intensity_max: double (nullable = true)\n",
            " |-- precip_max_time: timestamp (nullable = true)\n",
            " |-- precip_prob: double (nullable = true)\n",
            " |-- precip_type: string (nullable = true)\n",
            " |-- pressure: double (nullable = true)\n",
            " |-- summary: string (nullable = true)\n",
            " |-- temperature: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col, dayofweek, month, year\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import (\n",
        "    StringIndexer,\n",
        "    OneHotEncoder,\n",
        "    VectorAssembler\n",
        ")\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "\n",
        "class InsideSalesGBTRegression:\n",
        "    \"\"\"\n",
        "    Nonlinear regression pipeline (GBT) for inside_sales forecasting\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pipeline = None\n",
        "        self.model = None\n",
        "\n",
        "    def _feature_engineering(self, df: DataFrame) -> DataFrame:\n",
        "        return (\n",
        "            df\n",
        "            .withColumn(\"day_of_week\", dayofweek(col(\"date\")))\n",
        "            .withColumn(\"month\", month(col(\"date\")))\n",
        "            .withColumn(\"year\", year(col(\"date\")))\n",
        "        )\n",
        "\n",
        "    def build_pipeline(self) -> Pipeline:\n",
        "\n",
        "        categorical_cols = [\"precip_type\", \"summary\"]\n",
        "\n",
        "        numeric_cols = [\n",
        "            # operational\n",
        "            \"outside_sales\",\n",
        "            \"inside_covers\",\n",
        "            \"outside_covers\",\n",
        "            \"reserved_covers\",\n",
        "            \"walkin_covers\",\n",
        "            \"waitlist_covers\",\n",
        "            \"no_show_covers\",\n",
        "            \"no_show_parties\",\n",
        "\n",
        "            # weather\n",
        "            \"apparent_temperature\",\n",
        "            \"humidity\",\n",
        "            \"precip_intensity_max\",\n",
        "            \"precip_prob\",\n",
        "            \"pressure\",\n",
        "            \"temperature\",\n",
        "\n",
        "            # calendar\n",
        "            \"day_of_week\",\n",
        "            \"month\",\n",
        "            \"year\",\n",
        "\n",
        "        ]\n",
        "\n",
        "        indexers = [\n",
        "            StringIndexer(\n",
        "                inputCol=c,\n",
        "                outputCol=f\"{c}_idx\",\n",
        "                handleInvalid=\"keep\"\n",
        "            )\n",
        "            for c in categorical_cols\n",
        "        ]\n",
        "\n",
        "        encoders = [\n",
        "            OneHotEncoder(\n",
        "                inputCol=f\"{c}_idx\",\n",
        "                outputCol=f\"{c}_ohe\"\n",
        "            )\n",
        "            for c in categorical_cols\n",
        "        ]\n",
        "\n",
        "        assembler = VectorAssembler(\n",
        "            inputCols=numeric_cols + [f\"{c}_ohe\" for c in categorical_cols],\n",
        "            outputCol=\"features\",\n",
        "            handleInvalid=\"keep\"\n",
        "        )\n",
        "\n",
        "        gbt = GBTRegressor(\n",
        "            labelCol=\"inside_sales\",\n",
        "            featuresCol=\"features\",\n",
        "            maxDepth=6,\n",
        "            maxIter=150,\n",
        "            stepSize=0.05,\n",
        "            subsamplingRate=0.8,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        self.pipeline = Pipeline(\n",
        "            stages=indexers + encoders + [assembler, gbt]\n",
        "        )\n",
        "\n",
        "        return self.pipeline\n",
        "\n",
        "    def fit(self, df: DataFrame):\n",
        "        df = self._feature_engineering(df)\n",
        "        self.pipeline = self.build_pipeline()\n",
        "        self.model = self.pipeline.fit(df)\n",
        "        return self\n",
        "\n",
        "    def predict(self, df: DataFrame) -> DataFrame:\n",
        "        df = self._feature_engineering(df)\n",
        "        return self.model.transform(df)\n",
        "\n",
        "    def evaluate(self, df: DataFrame) -> dict:\n",
        "        preds = self.predict(df)\n",
        "\n",
        "        rmse = RegressionEvaluator(\n",
        "            labelCol=\"inside_sales\",\n",
        "            predictionCol=\"prediction\",\n",
        "            metricName=\"rmse\"\n",
        "        ).evaluate(preds)\n",
        "\n",
        "        r2 = RegressionEvaluator(\n",
        "            labelCol=\"inside_sales\",\n",
        "            predictionCol=\"prediction\",\n",
        "            metricName=\"r2\"\n",
        "        ).evaluate(preds)\n",
        "\n",
        "        return {\"RMSE\": rmse, \"R2\": r2}\n"
      ],
      "metadata": {
        "id": "bfiO8srfikkT"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time-based split (mandatory for forecasting)\n",
        "train_df = df_sales.filter(col(\"date\") < \"2018-01-01\")\n",
        "test_df  = df_sales.filter(col(\"date\") >= \"2018-01-01\")\n",
        "\n",
        "model = InsideSalesGBTRegression()\n",
        "model.fit(train_df)\n",
        "\n",
        "metrics = model.evaluate(test_df)\n",
        "print(metrics)\n",
        "\n",
        "preds = model.predict(test_df)\n",
        "preds.select(\"date\", \"inside_sales\", \"prediction\").show(10)\n"
      ],
      "metadata": {
        "id": "2Zy1ehOgipzr",
        "outputId": "5f805c58-0202-4392-cd7b-2c72e3095992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'RMSE': 2396.8506247214304, 'R2': 0.0025959482984100335}\n",
            "+----------+------------+------------------+\n",
            "|      date|inside_sales|        prediction|\n",
            "+----------+------------+------------------+\n",
            "|2018-01-01|       381.0|10466.190034976446|\n",
            "|2018-01-02|    11591.75| 11601.33193561661|\n",
            "|2018-01-03|    12052.11|14193.950460682361|\n",
            "|2018-01-04|    12296.98|13829.138665249837|\n",
            "|2018-01-05|    15831.14| 15697.66069566717|\n",
            "|2018-01-06|    17706.94| 17822.87842499601|\n",
            "|2018-01-07|    12013.59| 13303.11879410541|\n",
            "|2018-01-08|    10950.13|  13794.3483972079|\n",
            "|2018-01-09|    13713.21|13667.211944422634|\n",
            "|2018-01-10|    13153.22|13640.368901588176|\n",
            "+----------+------------+------------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpbTJU8Mgek2"
      },
      "source": [
        "# Generalized regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RhfkP7Kgezk",
        "outputId": "09bea76d-a035-4c1e-bc38-be8a16ade626"
      },
      "source": [
        "cuse = spark.read.csv('./cuse_binary.csv', header=True, inferSchema=True)\n",
        "cuse.show(5)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+---------+---+\n",
            "|age|education|wantsMore|  y|\n",
            "+---+---------+---------+---+\n",
            "|<25|      low|      yes|  0|\n",
            "|<25|      low|      yes|  0|\n",
            "|<25|      low|      yes|  0|\n",
            "|<25|      low|      yes|  0|\n",
            "|<25|      low|      yes|  0|\n",
            "+---+---------+---------+---+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYBQARF7ge2Z",
        "outputId": "6681eece-3854-485e-a7f6-743e65d9239d"
      },
      "source": [
        "cuse.columns[0:3]\n",
        "# cuse.select('age').distinct().show()\n",
        "cuse.select('age').rdd.countByValue()\n",
        "# cuse.select('education').rdd.countByValue()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {Row(age='<25'): 397,\n",
              "             Row(age='25-29'): 404,\n",
              "             Row(age='30-39'): 612,\n",
              "             Row(age='40-49'): 194})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "relu2PFBge5z",
        "outputId": "96e6e312-e338-4060-8c0b-de6be0d3d25a"
      },
      "source": [
        "# string index each categorical string columns\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=\"indexed_\"+column) for column in ('age', 'education', 'wantsMore')]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "indexed_cuse = pipeline.fit(cuse).transform(cuse)\n",
        "indexed_cuse.select('age', 'indexed_age').distinct().show(5)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+\n",
            "|  age|indexed_age|\n",
            "+-----+-----------+\n",
            "|30-39|        0.0|\n",
            "|  <25|        2.0|\n",
            "|25-29|        1.0|\n",
            "|40-49|        3.0|\n",
            "+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohO4WZWRg8-N",
        "outputId": "b1fb8f27-b421-4d14-b9dd-9db36ffb949a"
      },
      "source": [
        "# onehotencode each indexed categorical columns\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "columns = indexed_cuse.columns[0:3]\n",
        "onehoteencoders = [OneHotEncoder(inputCol=\"indexed_\"+column, outputCol=\"onehotencode_\"+column) for column in columns]\n",
        "pipeline = Pipeline(stages=onehoteencoders)\n",
        "onehotencode_columns = ['onehotencode_age', 'onehotencode_education', 'onehotencode_wantsMore', 'y']\n",
        "onehotencode_cuse = pipeline.fit(indexed_cuse).transform(indexed_cuse).select(onehotencode_columns)\n",
        "onehotencode_cuse.distinct().show(5)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------------------+----------------------+---+\n",
            "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|  y|\n",
            "+----------------+----------------------+----------------------+---+\n",
            "|   (3,[1],[1.0])|             (1,[],[])|         (1,[0],[1.0])|  0|\n",
            "|   (3,[2],[1.0])|         (1,[0],[1.0])|             (1,[],[])|  1|\n",
            "|   (3,[0],[1.0])|         (1,[0],[1.0])|         (1,[0],[1.0])|  0|\n",
            "|       (3,[],[])|         (1,[0],[1.0])|         (1,[0],[1.0])|  1|\n",
            "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|  0|\n",
            "+----------------+----------------------+----------------------+---+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjqDL8pOg_mU",
        "outputId": "314f1740-f72c-496c-982a-f51eeae773b4"
      },
      "source": [
        "# assemble all feature columns into on single vector column\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['onehotencode_age', 'onehotencode_education', 'onehotencode_wantsMore'], outputCol='features')\n",
        "cuse_df_2 = assembler.transform(onehotencode_cuse).withColumnRenamed('y', 'label')\n",
        "cuse_df_2.show(5)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------------------+----------------------+-----+-------------------+\n",
            "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label|           features|\n",
            "+----------------+----------------------+----------------------+-----+-------------------+\n",
            "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
            "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
            "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
            "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
            "|   (3,[2],[1.0])|             (1,[],[])|         (1,[0],[1.0])|    0|(5,[2,4],[1.0,1.0])|\n",
            "+----------------+----------------------+----------------------+-----+-------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coFKHYzTP-nL",
        "outputId": "479f835c-9689-44a0-899c-ea45fedaa61a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+---------------+-------------+---------------+--------------+---------------+--------------------+--------+--------------------+-------------------+-----------+-----------+--------+-------------------+-----------+-----------+-----+----------+----------+\n",
            "|      date|inside_sales|reserved_covers|walkin_covers|waitlist_covers|no_show_covers|no_show_parties|apparent_temperature|humidity|precip_intensity_max|    precip_max_time|precip_prob|precip_type|pressure|            summary|temperature|day_of_week|month|is_weekend|   date_ts|\n",
            "+----------+------------+---------------+-------------+---------------+--------------+---------------+--------------------+--------+--------------------+-------------------+-----------+-----------+--------+-------------------+-----------+-----------+-----+----------+----------+\n",
            "|2019-01-02|    12685.24|            132|            0|             51|             4|              1|               31.74|    0.61|                 0.0|2026-01-03 05:01:00|        0.0|       none|  1023.7|             cloudy|      36.51|          4|    1|         0|1546387200|\n",
            "|2019-01-03|    13580.96|            131|            0|             53|             6|              2|               37.38|    0.59|              1.0E-4|2026-01-03 07:00:00|        0.0|       none| 1016.86|partly-cloudy-night|      37.38|          5|    1|         0|1546473600|\n",
            "|2019-01-04|    16002.75|            138|            0|             59|             6|              2|               42.03|    0.76|              1.0E-4|2026-01-03 13:00:00|        0.0|       none| 1011.72|        clear-night|       43.4|          6|    1|         0|1546560000|\n",
            "|2019-01-05|    16463.62|            157|            0|             59|             0|              0|               40.82|     0.9|              0.0778|2026-01-03 03:00:00|       0.06|       rain| 1003.25|partly-cloudy-night|      44.58|          7|    1|         1|1546646400|\n",
            "|2019-01-06|    12405.14|            113|            2|             63|             8|              3|               30.14|    0.43|              1.0E-4|2026-01-03 12:00:00|        0.0|       none| 1021.61|partly-cloudy-night|      38.52|          1|    1|         1|1546732800|\n",
            "+----------+------------+---------------+-------------+---------------+--------------+---------------+--------------------+--------+--------------------+-------------------+-----------+-----------+--------+-------------------+-----------+-----------+-----+----------+----------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMuEWNjZhCus",
        "outputId": "ede3cf08-4829-4cbf-c62c-5e4fd7f3a4e0"
      },
      "source": [
        "# split data into training and test datasets\n",
        "training, test = cuse_df_2.randomSplit([0.8, 0.2], seed=1234)\n",
        "training.show(5)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------------------+----------------------+-----+---------+\n",
            "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label| features|\n",
            "+----------------+----------------------+----------------------+-----+---------+\n",
            "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
            "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
            "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
            "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
            "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|\n",
            "+----------------+----------------------+----------------------+-----+---------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q_IWGzXhE27"
      },
      "source": [
        "## ======= build cross validation model ===========\n",
        "\n",
        "# estimator\n",
        "from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "glm = GeneralizedLinearRegression(featuresCol='features', labelCol='label', family='binomial')\n",
        "\n",
        "# parameter grid\n",
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "param_grid = ParamGridBuilder().\\\n",
        "    addGrid(glm.regParam, [0, 0.5, 1, 2, 4]).\\\n",
        "    build()\n",
        "\n",
        "# evaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction')\n",
        "\n",
        "# build cross-validation model\n",
        "from pyspark.ml.tuning import CrossValidator\n",
        "cv = CrossValidator(estimator=glm, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=4)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ix5ugEvhJdF"
      },
      "source": [
        "# fit model\n",
        "# cv_model = cv.fit(training)\n",
        "cv_model = cv.fit(cuse_df_2)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(cv_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "H_8klxPMQoiv",
        "outputId": "0aead99d-096d-4a04-fb4a-4723882c76f7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.ml.tuning.CrossValidatorModel"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.ml.tuning.CrossValidatorModel</b><br/>def __init__(bestModel: Model, avgMetrics: Optional[List[float]]=None, subModels: Optional[List[List[Model]]]=None, stdMetrics: Optional[List[float]]=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/ml/tuning.py</a>CrossValidatorModel contains the model with the highest average cross-validation\n",
              "metric across folds and uses this model to transform input data. CrossValidatorModel\n",
              "also tracks the metrics for each param map evaluated.\n",
              "\n",
              ".. versionadded:: 1.4.0\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Since version 3.3.0, CrossValidatorModel contains a new attribute &quot;stdMetrics&quot;,\n",
              "which represent standard deviation of metrics for each paramMap in\n",
              "CrossValidator.estimatorParamMaps.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 1026);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4wyjSK7hMll",
        "outputId": "69681054-2694-47c2-91e3-bad2f2ecf0fc"
      },
      "source": [
        "# prediction\n",
        "pred_training_cv = cv_model.transform(training)\n",
        "pred_test_cv = cv_model.transform(test)\n",
        "\n",
        "pred_training_cv.show(5)\n",
        "pred_test_cv.show(5, truncate=False)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
            "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label| features|        prediction|\n",
            "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
            "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151407|\n",
            "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151407|\n",
            "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151407|\n",
            "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151407|\n",
            "|       (3,[],[])|             (1,[],[])|             (1,[],[])|    0|(5,[],[])|0.5140024065151407|\n",
            "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
            "only showing top 5 rows\n",
            "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
            "|onehotencode_age|onehotencode_education|onehotencode_wantsMore|label|features |prediction        |\n",
            "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
            "|(3,[],[])       |(1,[],[])             |(1,[],[])             |0    |(5,[],[])|0.5140024065151407|\n",
            "|(3,[],[])       |(1,[],[])             |(1,[],[])             |0    |(5,[],[])|0.5140024065151407|\n",
            "|(3,[],[])       |(1,[],[])             |(1,[],[])             |0    |(5,[],[])|0.5140024065151407|\n",
            "|(3,[],[])       |(1,[],[])             |(1,[],[])             |0    |(5,[],[])|0.5140024065151407|\n",
            "|(3,[],[])       |(1,[],[])             |(1,[],[])             |0    |(5,[],[])|0.5140024065151407|\n",
            "+----------------+----------------------+----------------------+-----+---------+------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6v6dHHXhP6i",
        "outputId": "08285b8d-46c7-4c41-adf5-01f229601e17"
      },
      "source": [
        "cv_model.bestModel.coefficients"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseVector([-0.2806, -0.7999, -1.1892, 0.325, -0.833])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v9AZUSRhP-7",
        "outputId": "df6b05eb-7bdf-479c-9678-0d1210c55ca7"
      },
      "source": [
        "cv_model.bestModel.intercept"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05602427516928616"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdJGL9ZrhXm1",
        "outputId": "b125ae90-427a-4573-f87f-8c08ed65a9f5"
      },
      "source": [
        "evaluator.evaluate(pred_training_cv)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6716478245974649"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPJ2YTkuhhEE",
        "outputId": "87a91ab3-8bb6-4496-c8b5-79281dc22b90"
      },
      "source": [
        "evaluator.evaluate(pred_test_cv)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6830864197530864"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s2dyqB1hqkM"
      },
      "source": [],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "1. Do the generalized regression to forecast the `inside_sale` of this data: https://github.com/maks-p/restaurant_sales_forecasting/blob/master/csv/CSV_for_EDA_NEW.csv\n",
        "\n",
        "2. Wrap your code in a pipeline as a Python class"
      ],
      "metadata": {
        "id": "MHh2ALPnBLNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('./CSV_for_EDA_NEW.csv', header=True, inferSchema=True)\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "POG5SPMmgNe8",
        "outputId": "bc369c01-88ef-433a-a033-48dd9c209af3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- inside_sales: double (nullable = true)\n",
            " |-- outside_sales: double (nullable = true)\n",
            " |-- inside_covers: integer (nullable = true)\n",
            " |-- outside_covers: integer (nullable = true)\n",
            " |-- reserved_covers: integer (nullable = true)\n",
            " |-- walkin_covers: integer (nullable = true)\n",
            " |-- waitlist_covers: integer (nullable = true)\n",
            " |-- no_show_covers: integer (nullable = true)\n",
            " |-- no_show_parties: integer (nullable = true)\n",
            " |-- apparent_temperature: double (nullable = true)\n",
            " |-- humidity: double (nullable = true)\n",
            " |-- precip_intensity_max: double (nullable = true)\n",
            " |-- precip_max_time: timestamp (nullable = true)\n",
            " |-- precip_prob: double (nullable = true)\n",
            " |-- precip_type: string (nullable = true)\n",
            " |-- pressure: double (nullable = true)\n",
            " |-- summary: string (nullable = true)\n",
            " |-- temperature: double (nullable = true)\n",
            "\n",
            "+----------+------------+-------------+-------------+--------------+---------------+-------------+---------------+--------------+---------------+--------------------+--------+--------------------+-------------------+-----------+-----------+--------+-------------------+-----------+\n",
            "|      date|inside_sales|outside_sales|inside_covers|outside_covers|reserved_covers|walkin_covers|waitlist_covers|no_show_covers|no_show_parties|apparent_temperature|humidity|precip_intensity_max|    precip_max_time|precip_prob|precip_type|pressure|            summary|temperature|\n",
            "+----------+------------+-------------+-------------+--------------+---------------+-------------+---------------+--------------+---------------+--------------------+--------+--------------------+-------------------+-----------+-----------+--------+-------------------+-----------+\n",
            "|2017-01-02|    13159.84|          0.0|          174|             0|            106|           26|             42|            17|              6|               35.58|    0.92|              0.0242|2026-01-03 15:00:00|       0.79|       rain| 1027.98|               rain|      39.06|\n",
            "|2017-01-03|    12442.11|          0.0|          181|             0|            119|           31|             31|            14|              4|                41.5|    0.94|              0.0913|2026-01-03 17:00:00|       0.77|       rain| 1000.08|               rain|      43.19|\n",
            "|2017-01-04|    12927.64|          0.0|          174|             0|            131|           17|             26|             5|              2|               36.81|     0.4|              0.0176|2026-01-03 00:00:00|        0.0|       rain| 1002.55|        clear-night|      42.29|\n",
            "|2017-01-05|    14457.79|          0.0|          191|             0|            138|           25|             28|             4|              2|               27.11|    0.48|                 0.0|2026-01-03 05:01:00|        0.0|       none| 1014.61|             cloudy|      31.36|\n",
            "|2017-01-06|    15331.97|          0.0|          200|             0|            130|           16|             54|             6|              3|               24.74|    0.48|              0.0019|2026-01-03 12:00:00|        0.0|       snow|  1022.8|partly-cloudy-night|      29.42|\n",
            "+----------+------------+-------------+-------------+--------------+---------------+-------------+---------------+--------------+---------------+--------------------+--------+--------------------+-------------------+-----------+-----------+--------+-------------------+-----------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import lag\n",
        "\n",
        "w = Window.orderBy(\"date\")\n",
        "\n",
        "df = (\n",
        "    df\n",
        "    .withColumn(\"inside_sales_lag_1\", lag(\"inside_sales\", 1).over(w))\n",
        "    .withColumn(\"inside_sales_lag_7\", lag(\"inside_sales\", 7).over(w))\n",
        "    .withColumn(\"inside_sales_lag_14\", lag(\"inside_sales\", 14).over(w))\n",
        ")\n",
        "\n",
        "df = df.dropna(subset=[\n",
        "    \"inside_sales_lag_1\",\n",
        "    \"inside_sales_lag_7\",\n",
        "    \"inside_sales_lag_14\"\n",
        "])\n"
      ],
      "metadata": {
        "id": "nSxu4NiIhQ-Z"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "w = Window.orderBy(\"date\")\n",
        "\n",
        "df = (\n",
        "    df\n",
        "    .withColumn(\"sales_ma_7\", avg(\"inside_sales\").over(w.rowsBetween(-7, -1)))\n",
        "    .withColumn(\"sales_ma_14\", avg(\"inside_sales\").over(w.rowsBetween(-14, -1)))\n",
        "    .withColumn(\"sales_ma_28\", avg(\"inside_sales\").over(w.rowsBetween(-28, -1)))\n",
        ")\n",
        "\n",
        "df = df.dropna(subset=[\"sales_ma_28\"])\n"
      ],
      "metadata": {
        "id": "ZpxcZ6S2hrHm"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col, dayofweek, month, year\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import (\n",
        "    StringIndexer,\n",
        "    OneHotEncoder,\n",
        "    VectorAssembler,\n",
        "    StandardScaler\n",
        ")\n",
        "from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "\n",
        "class InsideSalesGLMPipeline:\n",
        "    \"\"\"\n",
        "    Generalized Linear Regression pipeline for forecasting inside_sales.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.pipeline = None\n",
        "\n",
        "    def _feature_engineering(self, df: DataFrame) -> DataFrame:\n",
        "        \"\"\"\n",
        "        Create time-based features from date.\n",
        "        \"\"\"\n",
        "        return (\n",
        "            df\n",
        "            .withColumn(\"day_of_week\", dayofweek(col(\"date\")))\n",
        "            .withColumn(\"month\", month(col(\"date\")))\n",
        "            .withColumn(\"year\", year(col(\"date\")))\n",
        "        )\n",
        "\n",
        "    def build_pipeline(self) -> Pipeline:\n",
        "        \"\"\"\n",
        "        Define Spark ML Pipeline.\n",
        "        \"\"\"\n",
        "\n",
        "        categorical_cols = [\"precip_type\", \"summary\"]\n",
        "        numeric_cols = [\n",
        "            \"outside_sales\",\n",
        "            \"inside_covers\",\n",
        "            \"outside_covers\",\n",
        "            \"reserved_covers\",\n",
        "            \"walkin_covers\",\n",
        "            \"waitlist_covers\",\n",
        "            \"no_show_covers\",\n",
        "            \"no_show_parties\",\n",
        "            \"apparent_temperature\",\n",
        "            \"humidity\",\n",
        "            \"precip_intensity_max\",\n",
        "            \"precip_prob\",\n",
        "            \"pressure\",\n",
        "            \"temperature\",\n",
        "            \"day_of_week\",\n",
        "            \"month\",\n",
        "            \"year\",\n",
        "            \"inside_sales_lag_1\",\n",
        "            \"inside_sales_lag_7\",\n",
        "            \"inside_sales_lag_14\",\n",
        "            \"sales_ma_7\",\n",
        "            \"sales_ma_14\",\n",
        "            \"sales_ma_28\"\n",
        "        ]\n",
        "\n",
        "        indexers = [\n",
        "            StringIndexer(\n",
        "                inputCol=c,\n",
        "                outputCol=f\"{c}_idx\",\n",
        "                handleInvalid=\"keep\"\n",
        "            )\n",
        "            for c in categorical_cols\n",
        "        ]\n",
        "\n",
        "        encoders = [\n",
        "            OneHotEncoder(\n",
        "                inputCol=f\"{c}_idx\",\n",
        "                outputCol=f\"{c}_ohe\"\n",
        "            )\n",
        "            for c in categorical_cols\n",
        "        ]\n",
        "\n",
        "        assembler = VectorAssembler(\n",
        "            inputCols=numeric_cols + [f\"{c}_ohe\" for c in categorical_cols],\n",
        "            outputCol=\"raw_features\",\n",
        "            handleInvalid=\"keep\"\n",
        "        )\n",
        "\n",
        "        scaler = StandardScaler(\n",
        "            inputCol=\"raw_features\",\n",
        "            outputCol=\"features\",\n",
        "            withMean=True,\n",
        "            withStd=True\n",
        "        )\n",
        "\n",
        "        glm = GeneralizedLinearRegression(\n",
        "            featuresCol=\"features\",\n",
        "            labelCol=\"inside_sales\",\n",
        "            family=\"gaussian\",\n",
        "            link=\"identity\",\n",
        "            maxIter=100,\n",
        "            regParam=0.0\n",
        "        )\n",
        "\n",
        "        self.pipeline = Pipeline(stages=indexers + encoders + [\n",
        "            assembler,\n",
        "            scaler,\n",
        "            glm\n",
        "        ])\n",
        "\n",
        "        return self.pipeline\n",
        "\n",
        "    def fit(self, df: DataFrame):\n",
        "        \"\"\"\n",
        "        Train GLM model.\n",
        "        \"\"\"\n",
        "        df = self._feature_engineering(df)\n",
        "        pipeline = self.build_pipeline()\n",
        "        self.model = pipeline.fit(df)\n",
        "        return self\n",
        "\n",
        "    def predict(self, df: DataFrame) -> DataFrame:\n",
        "        \"\"\"\n",
        "        Generate predictions.\n",
        "        \"\"\"\n",
        "        if not self.model:\n",
        "            raise ValueError(\"Model has not been trained\")\n",
        "\n",
        "        df = self._feature_engineering(df)\n",
        "        return self.model.transform(df)\n",
        "\n",
        "    def evaluate(self, df: DataFrame) -> dict:\n",
        "        \"\"\"\n",
        "        Evaluate model performance.\n",
        "        \"\"\"\n",
        "        preds = self.predict(df)\n",
        "\n",
        "        evaluator_rmse = RegressionEvaluator(\n",
        "            labelCol=\"inside_sales\",\n",
        "            predictionCol=\"prediction\",\n",
        "            metricName=\"rmse\"\n",
        "        )\n",
        "\n",
        "        evaluator_r2 = RegressionEvaluator(\n",
        "            labelCol=\"inside_sales\",\n",
        "            predictionCol=\"prediction\",\n",
        "            metricName=\"r2\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"RMSE\": evaluator_rmse.evaluate(preds),\n",
        "            \"R2\": evaluator_r2.evaluate(preds)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Ig44VM2fBSM0"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train / test split\n",
        "train_df = df.filter(col(\"date\") < \"2018-01-01\")\n",
        "test_df  = df.filter(col(\"date\") >= \"2018-01-01\")\n",
        "\n",
        "# Train model\n",
        "glm_pipeline = InsideSalesGLMPipeline()\n",
        "glm_pipeline.fit(train_df)\n",
        "\n",
        "# Evaluate\n",
        "metrics = glm_pipeline.evaluate(test_df)\n",
        "print(metrics)\n",
        "\n",
        "# Predict\n",
        "predictions = glm_pipeline.predict(test_df)\n",
        "predictions.select(\"date\", \"inside_sales\", \"prediction\").show(10)\n"
      ],
      "metadata": {
        "id": "VokA7TE_guIP",
        "outputId": "e933f8bb-cec9-4f82-a527-1a70ad290df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'RMSE': 2131.825719290717, 'R2': 0.21097169153599182}\n",
            "+----------+------------+------------------+\n",
            "|      date|inside_sales|        prediction|\n",
            "+----------+------------+------------------+\n",
            "|2018-01-01|       381.0|6165.9187488975385|\n",
            "|2018-01-02|    11591.75|11927.472049284357|\n",
            "|2018-01-03|    12052.11|13228.789365408986|\n",
            "|2018-01-04|    12296.98|13139.538909152223|\n",
            "|2018-01-05|    15831.14|15273.228230431521|\n",
            "|2018-01-06|    17706.94|16612.414951696825|\n",
            "|2018-01-07|    12013.59|14324.813374743033|\n",
            "|2018-01-08|    10950.13|13210.739667998238|\n",
            "|2018-01-09|    13713.21|13488.544072966377|\n",
            "|2018-01-10|    13153.22|14691.720213903463|\n",
            "+----------+------------+------------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    }
  ]
}