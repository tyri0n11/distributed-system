{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyri0n11/distributed-system/blob/main/lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise I\n",
        "\n",
        "The input is a textual csv file containing the daily value of PM10 for a set of sensors, and in each line of the files has the following format:\n",
        "```sensorId,date,PM10 value (Î¼g/m3)\\n```\n",
        "\n",
        "Here is the example of data:\n",
        "```\n",
        "s1,2016-01-01,20.5\n",
        "s2,2016-01-01,30.1\n",
        "s1,2016-01-02,60.2\n",
        "s2,2016-01-02,20.4\n",
        "s1,2016-01-03,55.5\n",
        "s2,2016-01-03,52.5\n",
        "```\n",
        "\n",
        "You're required to use pyspark to load the file, filter the values and use map/reduce code idea to give the output. The output is a line for each sensor on the standard output.\n",
        "Each line contains a `sensorId` and the list of `dates` with a PM10 values greater than 50 for that sensor. The example output:\n",
        "```\n",
        "(s1, [2016-01-02, 2016-01-03])\n",
        "(s2, [2016-01-03])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "8H58RnChZq0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"PM10_Filter\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Load CSV as RDD (map/reduce style instead of DataFrame SQL)\n",
        "rdd = sc.textFile(\"sensor.csv\")\n",
        "\n",
        "# Split lines by comma\n",
        "# Format: (sensorId, date, pm10)\n",
        "parsed_rdd = rdd.map(lambda line: line.split(\",\"))\n",
        "\n",
        "# Keep only rows with PM10 > 50\n",
        "filtered_rdd = parsed_rdd.filter(lambda x: float(x[2]) > 50)\n",
        "\n",
        "# Map to (sensorId, date)\n",
        "mapped_rdd = filtered_rdd.map(lambda x: (x[0], x[1]))\n",
        "\n",
        "# Reduce: collect list of dates for each sensor\n",
        "result_rdd = mapped_rdd.groupByKey().map(lambda x: (x[0], list(x[1])))\n",
        "\n",
        "# Print result\n",
        "for row in result_rdd.collect():\n",
        "    print(row)\n"
      ],
      "metadata": {
        "id": "z8WfOgK7N3f0",
        "outputId": "ee3933c9-c760-4da9-d5dd-9d233a4536f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('s1', ['2016-01-02', '2016-01-03', '2016-01-04', '2016-01-07'])\n",
            "('s3', ['2016-01-05', '2016-01-06', '2016-01-08', '2016-01-09'])\n",
            "('s2', ['2016-01-03', '2016-01-07', '2016-01-09'])\n",
            "('s4', ['2016-01-05', '2016-01-06', '2016-01-10'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise II\n",
        "\n",
        "Using the same data of the Exercise I, you're required to get the output: sensors ordered by the number of critical days. Each line of the output contains the number of days with a PM10 values greater than 50 for a sensor `s` and the `sensorId` of sensor `s`.\n",
        "\n",
        "The example of the output:\n",
        "```\n",
        "2, s1\n",
        "1, s2\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Jgu0vQKVbqDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Exercise_2\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Load CSV as RDD (map/reduce style instead of DataFrame SQL)\n",
        "rdd = sc.textFile(\"sensor.csv\")\n",
        "\n",
        "# Split lines by comma\n",
        "# Format: (sensorId, date, pm10)\n",
        "parsed_rdd = rdd.map(lambda line: line.split(\",\"))\n",
        "\n",
        "# Keep only rows with PM10 > 50\n",
        "filtered_rdd = parsed_rdd.filter(lambda x: float(x[2]) > 50)\n",
        "\n",
        "# Map to key-value: (sensorId, 1)\n",
        "sensor_count_rdd = filtered_rdd.map(lambda x: (x[0], 1))\n",
        "\n",
        "# Reduce by key to count how many times each sensorId appears\n",
        "result_rdd = sensor_count_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print in desired format: count, sensorId\n",
        "for count, sensorId in result_rdd.map(lambda x: (x[1], x[0])).collect():\n",
        "    print(f\"{count}, {sensorId}\")\n"
      ],
      "metadata": {
        "id": "8P_Sj4fFXIPu",
        "outputId": "a0e92b41-d5ac-44e4-82cb-ef71af8b9db3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4, s1\n",
            "4, s3\n",
            "3, s2\n",
            "3, s4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise III\n",
        "\n",
        "In this exercise, you're given an input: A CSV file containing a list of profiles\n",
        "\n",
        "- Header: `name,age,gender`\n",
        "- Each line of the file contains the information about one user\n",
        "\n",
        "The example of input data\n",
        "```\n",
        "name,surname,age\n",
        "Paolo,Garza,42\n",
        "Luca,Boccia,41\n",
        "Maura,Bianchi,16\n",
        "```\n",
        "\n",
        "You're required to use pyspark to load and analyze the data to achieve the output: A CSV file containing one line for each profile. The original age attribute is substituted with a new attributed called rangeage of type String.\n",
        "```\n",
        "rangeage = \"[\" + (age/10)*10 + \"-\" + (age/10)*10+9 + \"]\"\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ADGjGNWKePfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,concat,lit,floor\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Exercise_3\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "df = spark.read.csv(\"person.csv\", header=True)\n",
        "\n",
        "df = df.withColumnRenamed('_c0','name')\\\n",
        "        .withColumnRenamed('_c1','surname')\\\n",
        "        .withColumnRenamed('_c2','age')\n",
        "\n",
        "# Convert age to integer\n",
        "df = df.withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
        "\n",
        "# Compute rangeage\n",
        "df = df.withColumn(\n",
        "    \"rangeage\",\n",
        "    concat(\n",
        "        lit(\"[\"),\n",
        "        floor(col(\"age\") / 10) * 10,\n",
        "        lit(\"-\"),\n",
        "        (floor(col(\"age\") / 10) * 10) + 9,\n",
        "        lit(\"]\")\n",
        "    )\n",
        ")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "igKez2B6aPe0",
        "outputId": "19d95a4d-e156-4071-da12-2961aca2edfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+---+--------+\n",
            "| name| surname|age|rangeage|\n",
            "+-----+--------+---+--------+\n",
            "|Paolo|   Garza| 42| [40-49]|\n",
            "| Luca|  Boccia| 41| [40-49]|\n",
            "|Maura| Bianchi| 16| [10-19]|\n",
            "|Alice|   Cochi| 17| [10-19]|\n",
            "|Laura|  Latini| 28| [20-29]|\n",
            "|Paula| Zachini| 19| [10-19]|\n",
            "|Carta|  Cianci| 29| [20-29]|\n",
            "| Rita|Lisatini| 31| [30-39]|\n",
            "+-----+--------+---+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}